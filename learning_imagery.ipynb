{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/catarinacecilio/learning_imagery/blob/master/learning_imagery.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TPGZo-gG2nTO"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import sys\n",
        "import csv\n",
        "import multiprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Anotações\n",
        "\n",
        "\n",
        "1.   Nossa tabela tem '_NaN_' e 'None'\n",
        "2.   'None' é sem marcação por voluntário\n",
        "3.   '_NaN_' é marcação de nenhum \n",
        "4.   Temos que compreender cada dimensão das imagens (são diferentes por pasta)\n",
        "5.   Assim como alterar as dimensões das imagens para unificá-las, precisamos também fazer isso com os pontos proporcionalmente (mesmo resize)\n",
        "6.   Ideia: transformar os pontos de cada imagem em uma imagem colocar os pontos de onde tem penguim como um pixel \"pintado\" - 1 para penguim e 0 para sem penguim\n",
        "7.   Documento peng_size tem informação do tamanho do penguim em relação à câmera\n",
        "8. OpenCV https://pypi.org/project/opencv-python/\n",
        "9. Geometria pra ser o penguim\n",
        "10. Os modelos procuram por geometrias, talvez os pontos não sejam suficientes\n",
        "11. Vamos testar com os pontos e se não der certo, testar com as geometrias"
      ],
      "metadata": {
        "id": "MdSgqfcHOzTc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Raw Data"
      ],
      "metadata": {
        "id": "zoy5oZyy35mI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xvzf /content/NEKOa.tgz -C /content/raw_data\n",
        "!tar -xvzf /content/NEKOb.tgz -C /content/raw_data\n",
        "!tar -xvzf /content/NEKOc.tgz -C /content/raw_data\n",
        "!tar -xvzf /content/PETEc.tgz -C /content/raw_data\n",
        "!tar -xvzf /content/CompleteAnnotations_2016-07-11.tgz -C /content/raw_data"
      ],
      "metadata": {
        "id": "QXKWuXWY2wRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "roi_df = pd.read_csv('raw_data/roi.tsv', sep='\\,')\n",
        "camera_names = roi_df['Camera name']\n",
        "camera_names"
      ],
      "metadata": {
        "id": "qphcf1fJ27em"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# default format can be changed as needed\n",
        "def createFileList(myDir, format='.JPG'): \n",
        "    fileList = []\n",
        "    print(myDir)\n",
        "    labels = []\n",
        "    names = []\n",
        "    keywords = {\"K\" : \"1\",\"U\": \"0\",} # keys and values to be changed as needed\n",
        "    for root, dirs, files in os.walk(myDir, topdown=True):\n",
        "        for name in files:\n",
        "            if name.endswith(format):\n",
        "                fullName = os.path.join(root, name)\n",
        "                fileList.append(fullName)\n",
        "            for keyword in keywords:\n",
        "                if keyword in name:\n",
        "                    labels.append(keywords[keyword])\n",
        "                else:\n",
        "                    continue\n",
        "            names.append(name)\n",
        "    return fileList, labels, names"
      ],
      "metadata": {
        "id": "Xubz2AKSDCe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the original image\n",
        "myDir = '/content/raw_data/' #mudar\n",
        "myFileList, labels, names  = createFileList(myDir)\n",
        "n_files = len(names)"
      ],
      "metadata": {
        "id": "Jeogm2LPDjD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i = 0\n",
        "for file in myFileList:\n",
        "    print(file)\n",
        "    img_file = Image.open(file)\n",
        "    # img_file.show()\n",
        "# get original image parameters...\n",
        "    width, height = img_file.size\n",
        "    format = img_file.format\n",
        "    mode = img_file.mode\n",
        "# Make image Greyscale\n",
        "    img_grey = img_file.convert('L')\n",
        "    #img_grey.save('result.png')\n",
        "    #img_grey.show()\n",
        "# Save Greyscale values\n",
        "    value = np.asarray(img_grey.getdata(), dtype=np.int).reshape((width, height))\n",
        "    value = value.flatten()\n",
        "    \n",
        "    value = np.append(value,labels[i])\n",
        "    i +=1\n",
        "    \n",
        "    print(value)\n",
        "    with open(\"penguin_dataset.csv\", 'a') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(value)"
      ],
      "metadata": {
        "id": "krOpOR2_GfXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Função para coletar pontos nas fotos e transformar em dataframe"
      ],
      "metadata": {
        "id": "adLnxp6aOkfh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_to_list(data):\n",
        "    '''Função que transforma o .json em lista de records'''\n",
        "    list_of_records = []\n",
        "    temp_dict = {}\n",
        "  \n",
        "    if data['xy'] != None :\n",
        "        tamanho = 0\n",
        "        index_vol = 0\n",
        "        for i, voluntario in enumerate(data['xy']) :\n",
        "            if voluntario != None and voluntario != '_NaN_' :\n",
        "                if len(voluntario) > tamanho :\n",
        "                    tamanho = len(voluntario)\n",
        "                    index_vol = i\n",
        "                temp_dict['imName'] = data['imName']\n",
        "                temp_dict['biggest_record']  = data['xy'][i]\n",
        "        list_of_records.append(temp_dict)\n",
        "    \n",
        "    return list_of_records"
      ],
      "metadata": {
        "id": "VANlILdgPAFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_to_df(data):\n",
        "    '''Função que transforma o .json para dataframe'''\n",
        "    \n",
        "    list_of_records = transform_to_list(data)\n",
        "    base_df = pd.DataFrame([], columns=['imName', 'x', 'y'])\n",
        "    \n",
        "    for record in list_of_records :\n",
        "        if record != {} :\n",
        "            if record['biggest_record'] != None :\n",
        "                for xy in record['biggest_record'] :\n",
        "                    temp = {}\n",
        "                    temp['imName'] = record['imName']\n",
        "                    if type(xy) == list and len(xy) > 1 :\n",
        "                        temp['x'] = xy[0]\n",
        "                        temp['y'] = xy[1]\n",
        "                    elif xy != None and xy == '_NaN_' :\n",
        "                        temp['x'] = 'NaN'\n",
        "                        temp['y'] = 'NaN'\n",
        "                    base_df = base_df.append(temp, ignore_index=True)\n",
        "\n",
        "    return base_df"
      ],
      "metadata": {
        "id": "zbd83MjQOtCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Opening JSON file\n",
        "with open('sample_data/BAILa.json') as json_file:\n",
        "    data_HALFb = json.load(json_file)\n",
        "\n",
        "data_HALFb = data_HALFb['dots']"
      ],
      "metadata": {
        "id": "HVPsPc4BPdff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Código para processar em paralelo\n",
        "\n",
        "with multiprocessing.Pool(4) as p:\n",
        "    df_HALFb = p.map(transform_to_df, data_HALFb)\n"
      ],
      "metadata": {
        "id": "h9r3E3oQPeVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Como processamos em paralelo, o output da célula de cima é uma lista de dataframes,\n",
        "# aqui vai o código de concatenar dataframes em uma só\n",
        "\n",
        "new_df = pd.DataFrame([])\n",
        "\n",
        "for df in df_HALFb :\n",
        "    new_df = pd.concat([new_df, df], axis=0)"
      ],
      "metadata": {
        "id": "0MfDg65OP-PP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}